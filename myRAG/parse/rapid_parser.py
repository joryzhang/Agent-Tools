import sys
import os
import json
import hashlib
import re
import asyncio
import base64
from typing import List, Any, Dict, Optional
import fitz  # PyMuPDF
import pandas as pd
from rapidocr_onnxruntime import RapidOCR
from langchain_core.documents import Document
from langchain_core.messages import HumanMessage

# é…ç½®ç¯å¢ƒ
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from models import get_openai_virtual_model_client

from app.config import settings
from dotenv import load_dotenv

load_dotenv()

# ä¿®å¤ Windows ç¼–ç 
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')


class RapidPDFParser:
    def __init__(self, use_ocr: bool = True, full_ocr: bool = False):
        print("ğŸ”„ åˆå§‹åŒ– RapidOCR è§£æå™¨...")
        self.use_ocr = use_ocr
        self.full_ocr = full_ocr
        self.ocr = RapidOCR() if use_ocr else None

        self.img_dir = settings.IMG_OUTPUT_DIR
        self.cache_dir = settings.DATA_DIR / "cache"
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.img_dir, exist_ok=True)

        self.vlm_client = get_openai_virtual_model_client()
        self.semaphore = asyncio.Semaphore(5)

    def parse_pdf(self, pdf_path: str, file_name: str, ocr_threshold: int = 50) -> Dict[str, Any]:
        """
        [ä¸»å…¥å£] PDF è§£ææµç¨‹æ§åˆ¶å™¨
        é€»è¾‘ç°åœ¨å˜æˆäº†çº¿æ€§çš„æµæ°´çº¿ï¼Œéå¸¸æ¸…æ™°ã€‚
        """
        file_hash = self._get_file_hash(pdf_path)
        cache_path = self.cache_dir / f"{file_hash}_rapid_v2.json"

        # 1. ç¼“å­˜æ£€æŸ¥
        if cache_path.exists():
            print(f"â™»ï¸ è¯»å–ç¼“å­˜: {file_hash}")
            with open(cache_path, "r", encoding="utf-8") as f:
                return json.load(f)

        print(f"ğŸš€ å¼€å§‹è§£æ: {pdf_path}")
        doc = fitz.open(pdf_path)
        # [ä¿®å¤ç‚¹ 1] æå‰è·å–æ€»é¡µæ•°
        total_pages_count = len(doc)

        # 2. åˆå§‹åŒ–å®¹å™¨
        context = {
            "all_texts": [],
            "all_tables": [],
            "saved_images": [],
            "markdown_pages": [],  # ä»…å­˜å‚¨æ–‡æœ¬å’Œå›¾ç‰‡çš„ Markdownï¼ŒæŒ‰é¡µç´¢å¼•
            "file_hash": file_hash
        }

        # è¡¨æ ¼å¤„ç†çš„çŠ¶æ€æœº (Pending Buffer)
        table_state = {
            "pending": None,  # æš‚å­˜æ­£åœ¨å¤„ç†çš„è·¨é¡µè¡¨æ ¼
            "last_header": None  # ä¸Šä¸€é¡µçš„è¡¨å¤´
        }

        # 3. é€é¡µå¤„ç†å¾ªç¯
        for page_num, page in enumerate(doc):
            print(f"  ğŸ“„ Page {page_num + 1}/{len(doc)}")

            # --- A. æå–å¹¶å¤„ç†è¡¨æ ¼ (æ›´æ–° all_tables å’Œ table_state) ---
            # è¿”å› table_bboxes ç”¨äºåç»­æ“¦é™¤
            table_bboxes = self._process_page_tables(page, page_num, table_state, context["all_tables"])

            # --- B. æå–å›¾ç‰‡ (æ›´æ–° saved_images) ---
            # è¿”å›å›¾ç‰‡ç›¸å…³çš„ Markdown ç‰‡æ®µ
            img_md_list = self._process_page_images(page, page_num, context["file_hash"], context["saved_images"])

            # --- C. æ“¦é™¤è¡¨æ ¼åŒºåŸŸ ---
            self._redact_areas(page, table_bboxes)

            # --- D. æå–å‰©ä½™æ–‡æœ¬ (æ›´æ–° all_texts) ---
            # è¿”å›æ–‡æœ¬ç›¸å…³çš„ Markdown ç‰‡æ®µ
            text_md_list = self._process_page_text(page, page_num, ocr_threshold, context["all_texts"])

            # --- E. ç»„è£…å½“å‰é¡µçš„éè¡¨æ ¼ Markdown ---
            # æ³¨æ„ï¼šæ­¤æ—¶ä¸åŒ…å«è¡¨æ ¼ï¼Œè¡¨æ ¼æœ€åç»Ÿä¸€æ’
            page_content = f"## ç¬¬ {page_num + 1} é¡µ\n\n"

            # åˆå¹¶å›¾ç‰‡å’Œæ–‡æœ¬ï¼ŒæŒ‰å‚ç›´ä½ç½® y0 æ’åº (å¦‚æœéœ€è¦ä¸¥æ ¼æ’åºï¼Œå¯ä»¥åœ¨ process æ–¹æ³•é‡Œè¿”å›å¸¦åæ ‡çš„å…ƒç»„)
            # è¿™é‡Œç®€å•å¤„ç†ï¼šå…ˆæ”¾æ–‡æœ¬ï¼Œå›¾ç‰‡ç©¿æ’å…¶ä¸­(ç®€åŒ–ç‰ˆ)ï¼Œæˆ–è€…ç›´æ¥è¿½åŠ 
            # ä¸ºäº†ä¿æŒæ•´æ´ï¼Œæˆ‘ä»¬æŠŠåˆšæ‰è¿”å›çš„ list æ‹¼èµ·æ¥
            page_content += "\n".join(text_md_list + img_md_list)

            context["markdown_pages"].append(page_content)

        # 4. å¾ªç¯ç»“æŸåçš„æ”¶å°¾
        # å¦‚æœè¿˜æœ‰æ²¡æäº¤çš„è¡¨æ ¼ï¼Œå¼ºåˆ¶æäº¤
        if table_state["pending"]:
            self._commit_table(table_state["pending"], context["all_tables"])

        doc.close()

        # 5. [æ ¸å¿ƒæ­¥éª¤] å°†è¡¨æ ¼å›å¡«åˆ° Markdown ä¸­
        full_markdown = self._inject_tables_into_markdown(context["markdown_pages"], context["all_tables"])

        # 6. æ„é€ ç»“æœå¹¶ç¼“å­˜
        result = {
            "markdown": full_markdown,
            "texts": context["all_texts"],
            "tables": context["all_tables"],
            "saved_images": context["saved_images"],
            "metadata": {
                "file_name": file_name,
                "file_hash": file_hash,
                "total_pages": total_pages_count,
                "parser": "RapidOCR+PyMuPDF_Refactored"
            }
        }

        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        return result

    # ================= æ ¸å¿ƒé€»è¾‘æ‹†åˆ† =================

    def _process_page_tables(self, page, page_num: int, state: Dict, all_tables: List) -> List[Any]:
        """
        å¤„ç†å•é¡µè¡¨æ ¼ï¼šæå–ã€åˆå¹¶é€»è¾‘ã€æäº¤ã€‚
        è¿”å›è¡¨æ ¼çš„ bbox åˆ—è¡¨ï¼Œä¾›åç»­æ“¦é™¤ä½¿ç”¨ã€‚
        """
        tables = page.find_tables(snap_tolerance=3)
        bboxes = []

        # å¦‚æœæœ¬é¡µæ²¡è¡¨æ ¼ï¼Œè¯´æ˜ä¹‹å‰çš„è·¨é¡µè¡¨æ ¼è‚¯å®šæ–­äº†ï¼Œæäº¤å®ƒ
        if not tables and state["pending"]:
            self._commit_table(state["pending"], all_tables)
            state["pending"] = None

        for i, table in enumerate(tables):
            current_data = table.extract()
            if not current_data: continue
            bboxes.append(table.bbox)

            # --- è·¨é¡µåˆå¹¶åˆ¤å®šé€»è¾‘ ---
            is_merged = False
            pending = state["pending"]

            # åˆ¤å®šæ¡ä»¶ï¼šæœ‰Pending + æœ¬é¡µç¬¬ä¸€ä¸ª + ä½äºé¡¶éƒ¨ + åˆ—æ•°ä¸€è‡´
            if (pending and i == 0 and table.bbox[1] < 150 and
                    len(current_data[0]) == pending["cols_count"]):

                print(f"    ğŸ”— [åˆå¹¶] æ£€æµ‹åˆ°è·¨é¡µè¡¨æ ¼ (ç¬¬{page_num + 1}é¡µ)")

                first_row = current_data[0]

                # æƒ…å†µA: é‡å¤è¡¨å¤´ -> ä¸¢å¼ƒå½“å‰è¡¨å¤´
                if first_row == state["last_header"]:
                    current_data = current_data[1:]

                # æƒ…å†µB: å·¦ä¾§ä¸ºç©º -> æ‹¼æ¥åˆ°ä¸Šä¸€è¡Œ (è§£å†³æ–­è¡Œé—®é¢˜)
                elif str(first_row[0]).strip() == "":
                    last_row = pending["data"][-1]
                    for col_idx, cell_val in enumerate(first_row):
                        if cell_val and str(cell_val).strip():
                            # æ‹¼æ¥é€»è¾‘
                            pending["data"][-1][col_idx] = str(last_row[col_idx]) + str(cell_val)
                    current_data = current_data[1:]  # åˆ æ‰è¿™ä¸€è¡Œ

                if current_data:
                    pending["data"].extend(current_data)
                is_merged = True

            # --- æ–°è¡¨æ ¼å¤„ç† ---
            if not is_merged:
                # å…ˆæŠŠæ—§çš„å­˜äº†
                if state["pending"]:
                    self._commit_table(state["pending"], all_tables)

                # å¼€å¯æ–°çš„
                norm_data = self.normalize_table_header(current_data)
                state["pending"] = {
                    "data": norm_data,
                    "page_start": page_num + 1,
                    "cols_count": len(norm_data[0])
                }
                state["last_header"] = norm_data[0]

        return bboxes

    def _process_page_images(self, page, page_num: int, file_hash: str, saved_images: List) -> List[str]:
        """æå–å›¾ç‰‡ã€OCRã€ä¿å­˜ã€‚è¿”å› Markdown ç‰‡æ®µåˆ—è¡¨ã€‚"""
        md_snippets = []
        try:
            image_list = page.get_image_info(xrefs=True)
            for img_idx, img_info in enumerate(image_list):
                if img_info["size"] < 2048: continue  # å¿½ç•¥å°å›¾

                xref = img_info["xref"]
                base_image = page.parent.extract_image(xref)  # ä½¿ç”¨ parent (doc) æå–

                img_name = f"{file_hash}_p{page_num + 1}_img{img_idx + 1}.{base_image['ext']}"
                img_path = os.path.join(self.img_dir, img_name)

                with open(img_path, "wb") as f:
                    f.write(base_image["image"])

                # OCR è¯†åˆ«
                ocr_text = ""
                if self.use_ocr:
                    ocr_text = self._ocr_image(base_image["image"])

                saved_images.append({
                    "hash": file_hash,
                    "name": img_name,
                    "path": img_path,
                    "page": page_num + 1,
                    "description": ocr_text
                })

                if ocr_text.strip():
                    md_snippets.append(f"\n**[å›¾ç‰‡å†…å®¹]:** {ocr_text.strip()}\n")
        except Exception as e:
            print(f"    âš ï¸ å›¾ç‰‡å¤„ç†å‡ºé”™: {e}")

        return md_snippets

    def _process_page_text(self, page, page_num: int, ocr_threshold: int, all_texts: List) -> List[str]:
        """æå–æ–‡æœ¬ï¼ˆå« OCR è¡¥æ•‘ï¼‰ã€‚è¿”å› Markdown ç‰‡æ®µåˆ—è¡¨ã€‚"""
        md_snippets = []
        # get_text("blocks") æ­¤æ—¶å·²ç»æ˜¯æ“¦é™¤è¡¨æ ¼åçš„å†…å®¹äº†
        blocks = page.get_text("blocks")
        page_text = ""

        for block in blocks:
            # block: (x0, y0, x1, y1, text, ...)
            text = block[4].strip()
            if text:
                page_text += text + "\n"
                md_snippets.append(text)

        # OCR è¡¥æ•‘é€»è¾‘ (é’ˆå¯¹æ‰«æä»¶)
        if len(page_text) < ocr_threshold and self.full_ocr:
            print(f"    ğŸ” æ–‡æœ¬è¿‡å°‘ï¼Œå°è¯•æ•´é¡µ OCR...")
            pix = page.get_pixmap(dpi=200)
            ocr_result = self._ocr_image(pix.tobytes("png"))
            if len(ocr_result) > len(page_text) + 20:
                md_snippets = [ocr_result]  # æ›¿æ¢æ‰åŸæ¥çš„æ–‡æœ¬
                page_text = ocr_result

        if page_text.strip():
            all_texts.append({
                "page": page_num + 1,
                "type": "Text",
                "text": page_text.strip()
            })

        return md_snippets

    def _commit_table(self, pending: Dict, all_tables: List):
        """å°† Pending çš„è¡¨æ ¼è½¬ Markdown å¹¶å­˜å…¥ all_tables"""
        if not pending or not pending["data"]: return

        md_content = self._table_to_markdown(pending["data"])
        all_tables.append({
            "page": pending["page_start"],
            "content": md_content,
            "format": "markdown"
        })
        print(f"    âœ… æäº¤è¡¨æ ¼ (é¡µç : {pending['page_start']}, è¡Œæ•°: {len(pending['data'])})")

    def _inject_tables_into_markdown(self, markdown_pages: List[str], all_tables: List) -> str:
        """
        [æœ€åä¸€æ­¥] å°† all_tables é‡Œçš„è¡¨æ ¼å†…å®¹ï¼Œæ’å›åˆ°å¯¹åº”çš„ markdown_pages ä¸­ã€‚
        ç­–ç•¥ï¼šæ’åœ¨é¡µé¢æœ«å°¾ã€‚
        """
        for table in all_tables:
            page_idx = table["page"] - 1
            if 0 <= page_idx < len(markdown_pages):
                # è¿½åŠ åˆ°è¯¥é¡µæœ«å°¾
                markdown_pages[page_idx] += f"\n\n{table['content']}\n\n"

        return "\n\n".join(markdown_pages)

    def _redact_areas(self, page, bboxes):
        """æ“¦é™¤é¡µé¢ä¸Šçš„æŒ‡å®šåŒºåŸŸ"""
        if not bboxes: return
        for bbox in bboxes:
            page.add_redact_annot(bbox)
        page.apply_redactions()

    # ================= å·¥å…·å‡½æ•° =================

    def _ocr_image(self, img_bytes: bytes) -> str:
        if not self.ocr: return ""
        try:
            result, _ = self.ocr(img_bytes)
            if result: return "\n".join([line[1] for line in result])
        except:
            pass
        return ""

    def _get_file_hash(self, file_path: str) -> str:
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    @staticmethod
    def _table_to_markdown(table_data: List[List]) -> str:
        """è½¬ Markdown + æ¸…æ´—"""
        if not table_data: return ""
        try:
            df = pd.DataFrame(table_data[1:], columns=table_data[0])
        except:
            return ""  # ç»“æ„é”™è¯¯å…œåº•

        df = df.fillna("")
        # æ¸…æ´—æ¢è¡Œç¬¦
        df = df.map(lambda x: str(x).replace("\n", " ").replace("|", "\|").strip())
        return df.to_markdown(index=False, tablefmt="pipe")

    @staticmethod
    def normalize_table_header(data: List[List]) -> List[List]:
        """æ— å¤´è¡¨æ ¼æ³¨å…¥"""
        if not data: return []
        # ç®€å•åˆ¤å®šï¼šç¬¬ä¸€è¡Œå¦‚æœæœ‰è¶…é•¿æ–‡æœ¬ï¼Œè‚¯å®šä¸æ˜¯Header
        if any(len(str(x)) > 20 for x in data[0]):
            new_header = [f"åˆ—{i + 1}" for i in range(len(data[0]))]
            data.insert(0, new_header)
        return data

    async def process_images(self, file_info: Dict, images: List[Dict]) -> List[Document]:
        """
                å¯¹æå–çš„å›¾ç‰‡è¿›è¡Œè¯­ä¹‰åˆ†æï¼ˆå¼‚æ­¥å¹¶å‘ï¼‰
                ä½¿ç”¨ VLM æ¨¡å‹ç”Ÿæˆå›¾ç‰‡æè¿°
                """
        if not images:
            return []

        image_docs = []
        print(f"ğŸ–¼ï¸ å¼€å§‹è¯†åˆ« {len(images)} å¼ å›¾ç‰‡å†…å®¹ (ä½¿ç”¨ VLM)...")

        async def describe_single_image(img_dict):
            """å†…éƒ¨å‡½æ•°ï¼šå¤„ç†å•å¼ å›¾ç‰‡"""
            image_name = img_dict.get("name")
            image_path = img_dict.get("path")
            file_hash = file_info.get("file_hash")
            if not image_path or not os.path.exists(image_path):
                return None

            # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
            async with self.semaphore:
                # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º Base64 ç¼–ç 
                with open(image_path, "rb") as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode("utf-8")

                # æ„é€  Promptï¼ŒæŒ‡å¯¼æ¨¡å‹å¦‚ä½•æè¿°å›¾ç‰‡
                message = HumanMessage(
                    content=[
                        {"type": "text",
                         "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚å¦‚æœæ˜¯æ¶æ„å›¾ï¼Œè¯·è¯´æ˜ç»„ä»¶å…³ç³»ï¼›å¦‚æœæ˜¯æµç¨‹å›¾ï¼Œè¯·è¯´æ˜æ­¥éª¤ï¼›å¦‚æœæ˜¯ç…§ç‰‡ï¼Œè¯·æç‚¼æ ¸å¿ƒä¿¡æ¯ã€‚"},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{encoded_string}"},
                        },
                    ]
                )

                try:
                    # è°ƒç”¨ VLM æ¨¡å‹
                    response = await self.vlm_client.ainvoke([message])
                    description = response.content
                    ocr_result = img_dict.get("description")
                    if ocr_result and ocr_result.strip():
                        # è¿”å› Document å¯¹è±¡ï¼ŒåŒ…å«æè¿°å’Œå…ƒæ•°æ®
                        return Document(
                            page_content=f"[OCRè§£æç»“æœ]:{ocr_result}\n\n[å›¾ç‰‡è¯­ä¹‰åˆ†æ]: {description}",
                            metadata={
                                "file_hash": file_hash,
                                "file_name": image_name,
                                "type": "image",
                                "image_path": image_path,
                                "page": img_dict.get("page", 0),
                                "source": "pdf_extraction"
                            }
                        )
                    else:
                        return Document(
                            page_content=f"[å›¾ç‰‡è¯­ä¹‰åˆ†æ]: {description}",
                            metadata={
                                "file_hash": file_hash,
                                "file_name": image_name,
                                "type": "image",
                                "image_path": image_path,
                                "page": img_dict.get("page", 0),
                                "source": "pdf_extraction"
                            }
                        )
                except Exception as e:
                    print(f"âŒ å›¾ç‰‡è§£æå¤±è´¥ ({image_path}): {e}")
                    return None

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [describe_single_image(img) for img in images]
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks)

        # è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœï¼ˆNoneï¼‰
        image_docs = [doc for doc in results if doc is not None]
        print(f"âœ… å›¾ç‰‡è¯­ä¹‰åŒ–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(image_docs)} æ¡æè¿°ã€‚")
        return image_docs

    @staticmethod
    def to_documents(parse_result: Dict) -> List[Document]:
        """
        å°†è§£æç»“æœè½¬æ¢ä¸ºæ ‡å‡†çš„ LangChain Document å¯¹è±¡åˆ—è¡¨
        æ–¹ä¾¿åç»­å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“æˆ–ç”¨äº RAG æ£€ç´¢
        """
        documents = []
        file_name = parse_result["metadata"].get("file_name", "Unknown")
        file_hash = parse_result["metadata"].get("file_hash", "")
        # å°†æ¯ä¸ªæ–‡æœ¬å—è½¬æ¢ä¸ºä¸€ä¸ª Document
        for text_block in parse_result.get("texts", []):
            documents.append(Document(
                page_content=text_block["text"],
                metadata={
                    "file_name": file_name,
                    "file_hash": file_hash,
                    "type": text_block.get("type", "Text"),
                    "page": text_block.get("page", 0),
                    "source": "pdf_extraction"
                }
            ))

        # å°†æ¯ä¸ªè¡¨æ ¼è½¬æ¢ä¸ºä¸€ä¸ª Document
        for table in parse_result.get("tables", []):
            documents.append(Document(
                page_content=table["content"],
                metadata={
                    "file_hash": file_hash,
                    "type": "table",
                    "page": table.get("page", 0),
                    "format": "markdown",
                    "source": "pdf_extraction"
                }
            ))

        # for image in parse_result.get("saved_images", []):
        #     documents.append(Document(
        #         page_content=image["description"],
        #         metadata={
        #             "img_name": image.get("name", ""),
        #             "img_path": image.get("path", ""),
        #             "description": image.get("description", ""),
        #             "page": image.get("page", 0),
        #             "source": "pdf_extraction",
        #             "type": "image",
        #         }
        #     ))
        return documents


# å·¥å‚å‡½æ•°ä¿æŒä¸å˜
def get_pdf_parser(prefer_gpu: bool = True):
    return RapidPDFParser()
