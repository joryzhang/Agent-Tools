"""
è½»é‡çº§ PDF è§£æå™¨ - ä½¿ç”¨ RapidOCR + PyMuPDF
é€‚åˆ CPU ç¯å¢ƒå’Œèµ„æºå—é™çš„åœºæ™¯ï¼Œä½œä¸º Marker+Surya çš„é«˜æ•ˆæ›¿ä»£æ–¹æ¡ˆ
"""
# ==================== å¯¼å…¥å¿…è¦çš„åº“ ====================
import asyncio  # å¼‚æ­¥ I/O æ”¯æŒï¼Œç”¨äºå¹¶å‘å¤„ç†
import base64  # ç”¨äºå°†å›¾ç‰‡ç¼–ç ä¸º Base64 å­—ç¬¦ä¸²ä¾› VLM ä½¿ç”¨
import hashlib  # ç”¨äºè®¡ç®—æ–‡ä»¶ MD5 å“ˆå¸Œå€¼ï¼Œä½œä¸ºç¼“å­˜é”®
import re  # æ­£åˆ™è¡¨è¾¾å¼æ”¯æŒ
import os  # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œæ–‡ä»¶è·¯å¾„æ“ä½œ
import sys  # ç³»ç»Ÿç›¸å…³å‚æ•°
import json  # JSON æ•°æ®å¤„ç†ï¼Œç”¨äºç¼“å­˜å­˜å–
from typing import List, Any, Dict, Optional  # ç±»å‹æ ‡æ³¨æ”¯æŒ

import pandas as pd

# ==================== ä¿®å¤ Windows ç»ˆç«¯ç¼–ç é—®é¢˜ ====================
# Windows é»˜è®¤ç»ˆç«¯ç¼–ç å¯èƒ½å¯¼è‡´ä¸­æ–‡è¾“å‡ºä¹±ç ï¼Œè¿™é‡Œå¼ºåˆ¶è®¾ç½®ä¸º UTF-8
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')

# ==================== å¯¼å…¥ç¬¬ä¸‰æ–¹åº“ ====================
import fitz  # PyMuPDF åº“ï¼Œç”¨äºé«˜æ•ˆæå– PDF æ–‡æœ¬ã€å›¾ç‰‡å’Œè¡¨æ ¼ç»“æ„
from rapidocr_onnxruntime import RapidOCR  # RapidOCR åº“ï¼ŒåŸºäº ONNX Runtime çš„è½»é‡çº§ OCR å¼•æ“
from langchain_core.documents import Document  # LangChain æ–‡æ¡£å¯¹è±¡å®šä¹‰
from langchain_core.messages import HumanMessage  # LangChain æ¶ˆæ¯å¯¹è±¡å®šä¹‰

# ==================== è®¾ç½®é¡¹ç›®è·¯å¾„å¹¶å¯¼å…¥è‡ªå®šä¹‰æ¨¡å— ====================
# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° python pathï¼Œä»¥ä¾¿å¯¼å…¥ models å’Œ app æ¨¡å—
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from models import get_openai_virtual_model_client  # è·å– OpenAI å…¼å®¹çš„æ¨¡å‹å®¢æˆ·ç«¯
from app.config import DATA_DIR, IMG_OUTPUT_DIR  # å¯¼å…¥æ•°æ®å­˜å‚¨å’Œå›¾ç‰‡è¾“å‡ºç›®å½•é…ç½®
from dotenv import load_dotenv  # å¯¼å…¥ç¯å¢ƒå˜é‡åŠ è½½å·¥å…·

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()


class RapidPDFParser:
    """
    è½»é‡çº§ PDF è§£æå™¨ç±»
    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä½¿ç”¨ PyMuPDF å¿«é€Ÿæå–æ–‡æœ¬å±‚
    2. ä½¿ç”¨ PyMuPDF æŸ¥æ‰¾å’Œæå–è¡¨æ ¼
    3. æå– PDF ä¸­çš„å›¾ç‰‡
    4. ä½¿ç”¨ RapidOCR å¤„ç†æ‰«æç‰ˆé¡µé¢æˆ–å›¾ç‰‡ä¸­çš„æ–‡å­—
    5. ä½¿ç”¨ VLM (è§†è§‰è¯­è¨€æ¨¡å‹) ç”Ÿæˆå›¾ç‰‡æè¿°
    6. ä½¿ç”¨ LLM ç”Ÿæˆè¡¨æ ¼æ‘˜è¦
    """

    def __init__(self, use_ocr: bool = True, full_ocr: bool = False):
        """
        åˆå§‹åŒ–è§£æå™¨

        Args:
            use_ocr: æ˜¯å¦å¯ç”¨ OCR åŠŸèƒ½ã€‚å¦‚æœä¸º Trueï¼Œä¼šå¯¹æ‰«æé¡µå’Œå›¾ç‰‡è¿›è¡Œæ–‡å­—è¯†åˆ«ã€‚
        """
        print("ğŸ”„ æ­£åœ¨åˆå§‹åŒ– RapidOCR è§£æå™¨...")

        self.use_ocr = use_ocr
        self.full_ocr = full_ocr
        if use_ocr:
            # åˆå§‹åŒ– RapidOCR å¼•æ“ï¼Œè¿™ä¼šåŠ è½½ ONNX æ¨¡å‹
            # RapidOCR æ¯” Tesseract æ›´å¿«ä¸”å¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½
            self.ocr = RapidOCR()
        else:
            self.ocr = None

        # è®¾ç½®å›¾ç‰‡è¾“å‡ºç›®å½•
        self.img_dir = str(IMG_OUTPUT_DIR)
        # è®¾ç½®ç¼“å­˜ç›®å½•ï¼Œç”¨äºå­˜å‚¨è§£æç»“æœ JSON
        self.cache_dir = DATA_DIR / "cache"

        # ç¡®ä¿ç›®å½•å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.img_dir, exist_ok=True)

        # åˆå§‹åŒ– VLM å®¢æˆ·ç«¯ï¼Œç”¨äºåç»­çš„å›¾ç‰‡å†…å®¹ç†è§£
        self.vlm_client = get_openai_virtual_model_client()
        # åˆ›å»ºä¿¡å·é‡ï¼Œé™åˆ¶å¹¶å‘è¯·æ±‚æ•°ä¸º 5ï¼Œé¿å… API é™æµ
        self.semaphore = asyncio.Semaphore(5)

        print("âœ… è§£æå™¨åˆå§‹åŒ–å®Œæˆ")

    def _get_file_hash(self, file_path: str) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼
        ç”¨äºç”Ÿæˆç¼“å­˜æ–‡ä»¶åï¼Œç¡®åŒä¸€ä¸ªæ–‡ä»¶åªè§£æä¸€æ¬¡
        """
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            # åˆ†å—è¯»å–æ–‡ä»¶ï¼Œæ¯æ¬¡è¯»å– 4096 å­—èŠ‚ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨è¿‡å¤šå†…å­˜
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def _ocr_image(self, image_bytes: bytes) -> str:
        """
        å¯¹å•ä¸ªå›¾ç‰‡äºŒè¿›åˆ¶æ•°æ®è¿›è¡Œ OCR è¯†åˆ«

        Args:
            image_bytes: å›¾ç‰‡çš„äºŒè¿›åˆ¶æ•°æ®

        Returns:
            è¯†åˆ«å‡ºçš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœè¯†åˆ«å¤±è´¥åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        if not self.ocr:
            return ""
        try:
            # è°ƒç”¨ RapidOCR è¿›è¡Œè¯†åˆ«
            result, _ = self.ocr(image_bytes)
            if result:
                # result æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸€é¡¹åŒ…å« [åæ ‡, æ–‡æœ¬, ç½®ä¿¡åº¦] ä¸Šé¢result,_æ˜¯å¯¹ocrçš„è§£åŒ…ï¼Œåªå–æˆ‘ä»¬éœ€è¦çš„resultï¼Œåé¢çš„ç”¨_ä¸è¦
                # æˆ‘ä»¬åªéœ€è¦æå–æ–‡æœ¬å†…å®¹å¹¶ç”¨æ¢è¡Œç¬¦è¿æ¥
                return "\n".join([line[1] for line in result])
        except Exception as e:
            print(f"  âš ï¸ OCR è¯†åˆ«å¤±è´¥: {e}")
        return ""

    def _commit_table(self, pending_table: Dict, all_tables: List, page_elements: List):
        """
        [æ–°å¢] å°†ç¼“å†²åŒºè¡¨æ ¼æäº¤åˆ°ç»“æœé›†
        """
        if not pending_table or not pending_table["data"]:
            return

        # 1. è½¬ Markdown
        md_table = self._table_to_markdown(pending_table["data"])

        # 3. å­˜å…¥ all_tables
        all_tables.append({
            "page": pending_table["page_start"],
            "index": len(all_tables),
            "content": md_table,
            "format": "markdown"
        })

        # 4. (å¯é€‰) å°†å…¶æ’å…¥åˆ°é¡µé¢å…ƒç´ æµä¸­
        # æ³¨æ„ï¼šè¿™é‡Œä¼ è¿›æ¥çš„ page_elements æ˜¯å½“å‰é¡µçš„ã€‚
        # å¦‚æœè¿™ä¸ªè¡¨æ ¼è·¨é¡µäº†ï¼Œå®ƒåº”è¯¥å‡ºç°åœ¨å®ƒå¼€å§‹çš„é‚£ä¸€é¡µã€‚
        # è¿™æ˜¯ä¸€ä¸ªé€»è¾‘éš¾ç‚¹ã€‚ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬é€‰æ‹©ï¼š
        # **åªå°†è¡¨æ ¼è¿½åŠ åˆ°å®ƒå¼€å§‹çš„é‚£ä¸€é¡µçš„ Markdown æ–‡æœ¬æœ«å°¾**
        # æ‰€ä»¥è¿™é‡Œä¸éœ€è¦æ“ä½œ page_elementsï¼Œè€Œæ˜¯åœ¨ parse_pdf å¾ªç¯å¤–çš„å¤„ç†é€»è¾‘é‡Œåšã€‚

        print(f"    âœ… æäº¤è¡¨æ ¼ (é¡µç  {pending_table['page_start']}, è¡Œæ•° {len(pending_table['data'])})")

    def parse_pdf(self, pdf_path: str, file_name: Optional[str], ocr_threshold: int = 50) -> Dict[str, Any]:
        """
        æ‰§è¡Œ PDF è§£æçš„ä¸»æµç¨‹

        Args:
            pdf_path: PDF æ–‡ä»¶çš„è·¯å¾„
            ocr_threshold: OCR è§¦å‘é˜ˆå€¼ã€‚å¦‚æœæŸé¡µæå–çš„æ–‡æœ¬å­—ç¬¦æ•°å°‘äºæ­¤å€¼ï¼Œ
                           åˆ™è®¤ä¸ºè¯¥é¡µå¯èƒ½æ˜¯æ‰«æå›¾ç‰‡ï¼Œä¼šå°è¯•ä½¿ç”¨ OCRã€‚

        Returns:
            åŒ…å«æ‰€æœ‰è§£æç»“æœçš„å­—å…¸
        """
        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼Œç”¨äºç¼“å­˜æ§åˆ¶
        file_hash = self._get_file_hash(pdf_path)
        # æ„é€ ç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_path = self.cache_dir / f"{file_hash}_rapid.json"

        # -------------------- æ£€æŸ¥ç¼“å­˜ --------------------
        if cache_path.exists():
            print(f"â™»ï¸ å‘ç°ç¼“å­˜ï¼Œç›´æ¥è¯»å–: {file_hash}")
            with open(cache_path, "r", encoding="utf-8") as f:
                return json.load(f)

        print(f"ğŸš€ ä½¿ç”¨ RapidOCR+PyMuPDF è§£æ: {pdf_path}")

        # ä½¿ç”¨ PyMuPDF æ‰“å¼€ PDF æ–‡æ¡£
        doc = fitz.open(pdf_path)

        # åˆå§‹åŒ–ç»“æœå®¹å™¨
        all_texts = []  # å­˜å‚¨æ‰€æœ‰æ–‡æœ¬å—
        all_tables = []  # å­˜å‚¨æ‰€æœ‰è¡¨æ ¼
        saved_images = []  # å­˜å‚¨æ‰€æœ‰ä¿å­˜çš„å›¾ç‰‡ä¿¡æ¯
        markdown_parts = []  # å­˜å‚¨æ¯ä¸€é¡µè½¬æ¢åçš„ Markdown æ–‡æœ¬
        # === [æ ¸å¿ƒä¿®æ”¹ 1] åˆå§‹åŒ–è¡¨æ ¼ç¼“å†²åŒº ===
        # ç”¨äºå­˜å‚¨æ­£åœ¨å¤„ç†ã€å°šæœªç»“æŸçš„è¡¨æ ¼
        pending_table = {
            "data": None,  # List[List], ç´¯è®¡çš„è¡Œæ•°æ®
            "page_start": 0,  # è¡¨æ ¼å¼€å§‹çš„é¡µç 
            "bbox_last": None,  # ä¸Šä¸€é¡µè¡¨æ ¼ç‰‡æ®µçš„ bbox (ç”¨äºåˆ¤æ–­ä½ç½®è¿ç»­æ€§)
            "header": None,  # è¡¨å¤´
            "cols_count": 0  # åˆ—æ•°
        }
        # ç”¨äºè·¨é¡µè¡¨æ ¼ä¿®å¤ï¼šè®°å½•ä¸Šä¸€é¡µæœ€åä¸€ä¸ªè¡¨æ ¼çš„è¡¨å¤´
        last_table_header = None

        # -------------------- é€é¡µå¤„ç† --------------------
        for page_num in range(len(doc)):
            page = doc[page_num]  # è·å–ç¬¬ page_num é¡µå¯¹è±¡
            print(f"  ğŸ“„ å¤„ç†ç¬¬ {page_num + 1}/{len(doc)} é¡µ...")
            # è¿™ä¸€é¡µçš„å…ƒç´ æš‚å­˜åˆ—è¡¨ï¼š[(y0, x0, "content_type", "content")]
            page_elements = []

            # ========== 1. ä¼˜å…ˆæå–è¡¨æ ¼ (è·å–å†…å®¹å¹¶è®°å½•åŒºåŸŸ) ==========
            # è¿™æ˜¯ä¸€ä¸ªå…³é”®æ­¥éª¤ï¼šæˆ‘ä»¬å…ˆè¯†åˆ«è¡¨æ ¼ï¼Œç¨åä¼šæŠŠè¡¨æ ¼åŒºåŸŸâ€œæŠ¹é™¤â€ï¼Œ
            # è¿™æ ·æå–çº¯æ–‡æœ¬æ—¶å°±ä¸ä¼šåŒ…å«è¡¨æ ¼å†…å®¹çš„ä¹±ç ã€‚
            table_bboxes = []
            try:
                # find_tables() æ˜¯ PyMuPDF å†…ç½®çš„æ£€æµ‹åŠŸèƒ½
                # å¯ä»¥æé«˜å¯¹æœ‰çº¿è¡¨æ ¼çš„è¯†åˆ«ç‡ï¼Œå¢åŠ  snap_tolerance å¯ä»¥æé«˜å¯¹æ­ªæ–œè¡¨æ ¼çš„å®¹å¿åº¦
                tables = page.find_tables(snap_tolerance=3)

                # å¦‚æœæœ¬é¡µæ²¡æœ‰è¡¨æ ¼ï¼Œè¯´æ˜ä¸Šä¸€é¡µçš„ pending_table è‚¯å®šç»“æŸäº†
                if not tables:
                    self._commit_table(pending_table, all_tables, page_elements)
                    pending_table = {"data": None}  # é‡ç½®

                for i, table in enumerate(tables):
                    # extract() æå–è¡¨æ ¼å†…å®¹ä¸ºäºŒç»´åˆ—è¡¨
                    current_data = table.extract()
                    if not current_data: continue
                    # è®°å½•è¡¨æ ¼åœ¨é¡µé¢ä¸Šçš„è¾¹ç•Œæ¡† (ç”¨äºåç»­æ“¦é™¤)
                    table_box_to_confirm = table.bbox
                    table_bboxes.append(table.bbox)

                    # === [æ ¸å¿ƒä¿®æ”¹ 2] è·¨é¡µåˆå¹¶é€»è¾‘ ===
                    is_merged = False

                    # åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆå¹¶ï¼š
                    # 1. ç¼“å†²åŒºæœ‰æ•°æ®
                    # 2. å½“å‰æ˜¯æœ¬é¡µç¬¬ä¸€ä¸ªè¡¨æ ¼ (i==0)
                    # 3. å½“å‰è¡¨æ ¼ä½äºé¡µé¢é¡¶éƒ¨ (bbox.y0 < 100)
                    # 4. åˆ—æ•°ä¸€è‡´
                    if (pending_table["data"] and i == 0 and
                            table.bbox[1] < 150 and
                            len(current_data[0]) == pending_table["cols_count"]):
                        print(f"    ğŸ”— æ£€æµ‹åˆ°è·¨é¡µè¡¨æ ¼ (ç¬¬{page_num + 1}é¡µ)ï¼Œæ­£åœ¨åˆå¹¶...")
                        # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœæ¥ç»­é¡µçš„ç¬¬ä¸€è¡Œå·¦ä¾§ä¸ºç©º (å¦‚ä½ çš„å›¾3)ï¼Œè¯´æ˜æ˜¯ä¸Šä¸€è¡Œå†…å®¹çš„å»¶ç»­
                        # æˆ–è€…æ˜¯é‡å¤è¡¨å¤´ï¼Œéœ€è¦å»æ‰
                        first_row = current_data[0]

                        # æƒ…å†µ A: é‡å¤è¡¨å¤´ -> åˆ æ‰ç¬¬ä¸€è¡Œï¼Œç›´æ¥æ‹¼
                        if first_row == pending_table["header"]:
                            current_data = current_data[1:]
                        # æƒ…å†µ B: å·¦ä¾§ä¸ºç©º (å†…å®¹å»¶ç»­) -> æ‹¼æ¥åˆ°ä¸Šä¸€è¡Œ
                        elif str(first_row[0]).strip() == "":
                            # å–å‡º pending_table çš„æœ€åä¸€è¡Œ
                            last_row = pending_table["data"][-1]
                            # å°†å½“å‰é¡µç¬¬ä¸€è¡Œçš„éç©ºå†…å®¹æ‹¼æ¥åˆ°ä¸Šä¸€è¡Œå¯¹åº”åˆ—
                            for col_idx, cell_val in enumerate(first_row):
                                if cell_val and str(cell_val).strip():
                                    # æ‹¼æ¥æ–‡æœ¬
                                    pending_table["data"][-1][col_idx] = str(last_row[col_idx]) + str(cell_val)
                            # åˆ æ‰å·²ç»è¢«åˆå¹¶çš„ç¬¬ä¸€è¡Œ
                            current_data = current_data[1:]

                        # å°†å‰©ä½™è¡Œè¿½åŠ åˆ°ç¼“å†²åŒº
                        if current_data:
                            pending_table["data"].extend(current_data)

                        # æ›´æ–°çŠ¶æ€ï¼Œæ ‡è®°åˆå¹¶æˆåŠŸ
                        pending_table["bbox_last"] = table.bbox
                        is_merged = True

                    # === [æ ¸å¿ƒä¿®æ”¹ 3] æ–°è¡¨æ ¼å¤„ç† ===
                    if not is_merged:
                        # å¦‚æœä¹‹å‰æœ‰æ²¡ä¿å­˜çš„è¡¨æ ¼ï¼Œå…ˆä¿å­˜
                        if pending_table["data"]:
                            self._commit_table(pending_table, all_tables, page_elements)

                        # å¼€å§‹æ–°çš„ç¼“å†²åŒº
                        # 1. è§„èŒƒåŒ–è¡¨å¤´ (Headless check)
                        current_data = self.normalize_table_header(current_data)

                        pending_table = {
                            "data": current_data,
                            "page_start": page_num + 1,
                            "bbox_last": table.bbox,
                            "header": current_data[0],  # è®°å½•è¡¨å¤´ç”¨äºæ¯”å¯¹
                            "cols_count": len(current_data[0])
                        }
            except Exception as e:
                print(f"    âš ï¸ è¡¨æ ¼æå–å¤±è´¥: {e}")

            # ========== 2. æå–å›¾ç‰‡ (åœ¨æ“¦é™¤è¡¨æ ¼ä¹‹å‰) ==========
            # å¿…é¡»åœ¨æ“¦é™¤è¡¨æ ¼ä¹‹å‰åšï¼Œé˜²æ­¢è¡¨æ ¼èƒŒæ™¯å›¾æˆ–å…¶ä»–é‡å å…ƒç´ è¢«è¯¯åˆ 
            try:
                image_list = page.get_image_info(xrefs=True)
                for img_idx, img_info in enumerate(image_list):
                    xref = img_info["xref"]  # è·å–å›¾ç‰‡çš„äº¤å‰å¼•ç”¨ ID

                    # å°è¯•å¤„ç†å›¾ç‰‡ï¼ˆæœ‰äº›å›¾ç‰‡å¯èƒ½æ˜¯å¾ˆå¤šå°å—æ‹¼æˆçš„ï¼Œè¿™é‡Œç®€å•å¤„ç†ï¼‰
                    try:
                        base_image = doc.extract_image(xref)
                    except Exception:
                        continue

                    image_bytes = base_image["image"]
                    image_ext = base_image["ext"]

                    # è¿‡æ»¤å¤ªå°çš„å›¾æ ‡æˆ–çº¿æ¡ (å°äº 2KB)
                    if img_info["size"] < 2048:
                        continue

                    # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
                    img_name = f"{file_hash}_p{page_num + 1}_img{img_idx + 1}.{image_ext}"
                    img_path = os.path.join(self.img_dir, img_name)
                    img_bbox = img_info["bbox"]
                    with open(img_path, "wb") as f:
                        f.write(image_bytes)

                    current_image_data = {
                        "name": img_name,
                        "path": img_path,
                        "page": page_num + 1,
                        "description": None
                    }

                    # å¦‚æœå¼€å¯ OCRï¼Œå°è¯•è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—
                    img_text = None
                    if self.use_ocr:
                        img_text = self._ocr_image(image_bytes)
                        if img_text.strip():
                            current_image_data["description"] = img_text  # ç›´æ¥æ›´æ–°å­—å…¸
                            all_texts.append({
                                "page": page_num + 1,
                                "type": "ImageText",
                                "text": img_text,
                                "source": img_name
                            })
                    saved_images.append(current_image_data)  # å°†å®Œæ•´çš„å­—å…¸åŠ å…¥åˆ—è¡¨
                    page_elements.append((img_bbox[1], img_bbox[0], "image", img_text))
            except Exception as e:
                print(f"    âš ï¸ å›¾ç‰‡æå–å¤±è´¥: {e}")

            # ========== 3. æ“¦é™¤è¡¨æ ¼åŒºåŸŸ (Redaction) ==========
            # å°†è¯†åˆ«åˆ°çš„è¡¨æ ¼åŒºåŸŸä»é¡µé¢å†…å®¹ä¸­â€œç§»é™¤â€ï¼Œ
            # è¿™æ ·åç»­ get_text å°±ä¸ä¼šå†æ¬¡æå–åˆ°è¡¨æ ¼é‡Œçš„æ–‡å­—ã€‚
            if table_bboxes:
                for bbox in table_bboxes:
                    # æ·»åŠ æ“¦é™¤æ³¨é‡Š
                    page.add_redact_annot(bbox)
                # åº”ç”¨æ“¦é™¤ (content=False è¡¨ç¤ºä¸åˆ é™¤æ³¨é‡Šæœ¬èº«ï¼Œimages=0 è¡¨ç¤ºä¸åˆ é™¤å›¾ç‰‡)
                # å®é™…ä¸Š apply_redactions() é»˜è®¤ä¼šæ¸…é™¤å†…å®¹ã€‚
                # æ³¨æ„ï¼šè¿™ä¼šä¿®æ”¹ page å¯¹è±¡çš„å½“å‰çŠ¶æ€ï¼Œåç»­å¯¹è¯¥ page çš„ get_text åªèƒ½è·å–å‰©ä½™å†…å®¹ã€‚
                page.apply_redactions()

            # ========== 4. æå–å‰©ä½™æ–‡æœ¬ (Standard Text Extraction) ==========
            # data-mining æ¨¡å¼é€šå¸¸èƒ½æ›´å¥½ä¿ç•™æ®µè½ç»“æ„ è¿™é‡Œå–"blocks" è¿”å›æ ¼å¼: [(x0, y0, x1, y1, "lines", block_no, block_type)]
            text_blocks = page.get_text("blocks")
            page_text = ""
            for block in text_blocks:
                x0, y0, x1, y1, text, block_no, block_type = block
                page_text += text
                # è¿‡æ»¤æ‰ç©ºçš„æˆ–è€…åªæœ‰ç©ºç™½ç¬¦çš„å—
                if not text.strip(): continue

                # è¿™é‡Œçš„ text å·²ç»æ˜¯å»é™¤äº†è¡¨æ ¼å†…å®¹çš„çº¯æ–‡æœ¬äº†
                page_elements.append((y0, x0, "text", text.strip()))

            # ========== 5. è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦ OCR (é’ˆå¯¹å‰©ä½™åŒºåŸŸ) ==========
            # å¦‚æœå‰©ä½™æ–‡æœ¬å¾ˆå°‘ï¼Œæœ‰ä¸¤ç§æƒ…å†µï¼š
            # A. é¡µé¢æœ¬æ¥å°±æ˜¯ç©ºçš„æˆ–åªæœ‰è¡¨æ ¼ï¼ˆä¸éœ€è¦ OCRï¼‰
            # B. é¡µé¢æ˜¯æ‰«æä»¶ï¼Œè¡¨æ ¼æ£€æµ‹å¤±æ•ˆï¼ˆtable_bboxesä¸ºç©ºï¼‰ï¼Œéœ€è¦ OCR
            # C. é¡µé¢é™¤è¡¨æ ¼å¤–ç¡®å®æ²¡å­—äº† (ä¸éœ€è¦ OCR)

            # ç®€å•é€»è¾‘ï¼šå¦‚æœæ²¡æ‰¾åˆ°è¡¨æ ¼ ä¸” å­—æ•°å°‘ -> å¯èƒ½æ˜¯æ‰«æä»¶ -> OCR
            # å¦‚æœæ‰¾åˆ°äº†è¡¨æ ¼ï¼Œè¯´æ˜æ˜¯æ•°å­—æ–‡æ¡£ï¼Œå­—æ•°å°‘è¯´æ˜ç¡®å®æ²¡å­— -> ä¸ OCR
            if len(page_text.strip()) < ocr_threshold and self.full_ocr:
                # åªæœ‰å½“æ²¡æœ‰æ£€æµ‹åˆ°è¡¨æ ¼ï¼Œæˆ–è€…å¼ºåˆ¶ç­–ç•¥æ—¶æ‰ OCR
                # å¦‚æœæ£€æµ‹åˆ°äº†è¡¨æ ¼ï¼Œé€šå¸¸æ„å‘³ç€è¿™æ˜¯åŸç”Ÿ PDFï¼Œä¸éœ€è¦ OCR (é™¤éæ˜¯æ··åˆå‹)
                # ä½†ä¸ºäº†ä¿é™©ï¼Œå¦‚æœå­—æ•°æå°‘ï¼Œè¿˜æ˜¯æ£€æŸ¥ä¸€ä¸‹ã€‚
                # æ­¤æ—¶ page å·²ç»è¢« redact äº†è¡¨æ ¼ï¼Œå¦‚æœ OCR è¿™é‡Œï¼Œè¡¨æ ¼éƒ¨åˆ†ç”±ç™½å—æ›¿ä»£ï¼Œ
                # åˆšå¥½é¿å…äº†é‡å¤è¯†åˆ«è¡¨æ ¼å†…å®¹ã€‚

                print(f"    ğŸ” å‰©ä½™æ–‡æœ¬è¾ƒå°‘({len(page_text.strip())}å­—ç¬¦)ï¼Œå°è¯• OCR è¡¥å……...")

                # æ¸²æŸ“é¡µé¢ä¸ºå›¾ç‰‡ (æ³¨æ„ï¼šæ­¤æ—¶è¡¨æ ¼åŒºåŸŸå·²ç»æ˜¯ç©ºç™½äº†)
                pix = page.get_pixmap(dpi=200)
                img_bytes = pix.tobytes("png")

                ocr_text = self._ocr_image(img_bytes)
                if len(ocr_text) > len(page_text.strip()) + 10:  # å¦‚æœ OCR å¤šè¯†åˆ«å‡ºäº†æ˜¾è‘—å†…å®¹
                    print(f"    âœ… OCR è¡¥å……è¯†åˆ«åˆ° {len(ocr_text)} å­—ç¬¦")
                    page_elements = [e for e in page_elements if e[2] != "text"]
                    page_elements.append((0, 0, "ocr_text", ocr_text))  # OCRç»“æœé€šå¸¸ä½œä¸ºæ•´é¡µå—
                    page_text = ocr_text

            # ä¿å­˜æ–‡æœ¬å†…å®¹
            if page_text.strip():
                all_texts.append({
                    "page": page_num + 1,
                    "type": "Text",
                    "text": page_text.strip()
                })
            page_elements.sort(key=lambda x: (x[0], x[1]))
            # ç»„è£…è¿™ä¸€é¡µçš„ Markdown
            page_md_parts = [f"## ç¬¬ {page_num + 1} é¡µ"]
            for y0, x0, type_, content in page_elements:
                if type_ == "image":
                    # å¦‚æœå›¾ç‰‡ OCR è¯†åˆ«å‡ºäº†æ–‡å­—ï¼Œå°†å…¶åŒ…è£¹åœ¨å¼•ç”¨å—ä¸­ï¼Œå¹¶æ³¨æ˜æ¥æº
                    if content and content.strip():
                        page_md_parts.append(f"\n**[å›¾ç‰‡è¯†åˆ«å†…å®¹]:** {content.strip()}\n")
                elif type_ == "table":
                    # è¿™é‡Œæ˜¯ä¹‹å‰é€»è¾‘ç”Ÿæˆçš„ tableï¼Œç°åœ¨è¢« pending é€»è¾‘å–ä»£äº†
                    # åªæœ‰å½“ table è¢« commit æ—¶ï¼Œæˆ‘ä»¬æ‰æŠŠå®ƒåŠ å…¥ã€‚
                    # ç”±äºé€»è¾‘å¤æ‚ï¼Œæˆ‘å»ºè®®ï¼špage_elements é‡Œä¸å†æ”¾ table
                    # è€Œæ˜¯æŠŠ all_tables é‡Œçš„å†…å®¹ï¼Œæ ¹æ® page_num æ’è¿›å»ã€‚
                    pass
                    # æ–‡æœ¬å’Œä¿æŒåŸæ ·
                    page_md_parts.append(content)

            markdown_parts.append("\n\n".join(page_md_parts))

        # å¾ªç¯ç»“æŸåï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ®‹ç•™çš„è¡¨æ ¼
        if pending_table["data"]:
            self._commit_table(pending_table, all_tables, [])  # è¿™é‡Œä¼ ç©ºlistï¼Œå› ä¸ºæˆ‘ä»¬ä¸‹é¢ç»Ÿä¸€å¤„ç†

        # ä¿å­˜æ€»é¡µæ•°åå…³é—­æ–‡æ¡£
        total_pages = len(doc)
        doc.close()

        # ========== [æ ¸å¿ƒä¿®å¤] ç»Ÿä¸€å›å¡«è¡¨æ ¼åˆ° Markdown ==========
        # æ­¤æ—¶ markdown_parts å·²ç»åŒ…å«äº†æ¯ä¸€é¡µçš„æ–‡æœ¬å’Œå›¾ç‰‡
        # all_tables åŒ…å«äº†æ‰€æœ‰å¤„ç†å¥½çš„è¡¨æ ¼ï¼ˆå« page å­—æ®µï¼‰

        for table in all_tables:
            p_idx = table["page"] - 1  # è½¬æ¢ä¸ºåˆ—è¡¨ç´¢å¼•
            if 0 <= p_idx < len(markdown_parts):
                # ç­–ç•¥ï¼šå°†è¡¨æ ¼è¿½åŠ åˆ°è¯¥é¡µçš„æœ«å°¾
                # è¿™æ ·å¯ä»¥ä¿è¯è¡¨æ ¼ä¸ä¼šæ‰“æ–­æ®µè½ï¼Œä¸”ä¸Šä¸‹æ–‡è¿è´¯
                markdown_parts[p_idx] += f"\n\n{table['content']}\n\n"

        # ç»„åˆæ‰€æœ‰éƒ¨åˆ†çš„ Markdown
        full_markdown = "\n\n".join(markdown_parts)

        # æ„é€ æœ€ç»ˆç»“æœå­—å…¸
        result = {
            "markdown": full_markdown,
            "texts": all_texts,
            "tables": all_tables,
            "saved_images": saved_images,
            "metadata": {
                "file_name": file_name,
                "file_hash": file_hash,
                "total_pages": total_pages,
                "parser": "RapidOCR+PyMuPDF"
            }
        }

        # -------------------- å†™å…¥ç¼“å­˜ --------------------
        with open(cache_path, "w", encoding="utf-8") as f:
            # ensure_ascii=False ç¡®ä¿ä¸­æ–‡æ­£å¸¸æ˜¾ç¤ºï¼Œindent=2 æ ¼å¼åŒ–è¾“å‡º
            json.dump(result, f, ensure_ascii=False, indent=2)

        print(f"âœ… è§£æå®Œæˆ: æ–‡æœ¬å—[{len(all_texts)}], è¡¨æ ¼[{len(all_tables)}], å›¾ç‰‡[{len(saved_images)}]")
        print(result)
        return result

    @staticmethod
    def _table_to_markdown(table_data: List[List]) -> str:
        """
        å°† list of lists è½¬æ¢ä¸ºæ ‡å‡† Markdown è¡¨æ ¼ï¼Œå¹¶æ¸…æ´—æ¢è¡Œç¬¦ç­‰å™ªéŸ³ã€‚
        é’ˆå¯¹é•¿æ–‡æœ¬æ³•å¾‹æ–‡æ¡£ä¼˜åŒ–ã€‚
        """
        if not table_data:
            return ""
        # 1. è½¬æ¢ä¸º DataFrame
        try:
            # å‡è®¾ç¬¬ä¸€è¡Œæ˜¯ Header
            headers = table_data[0]
            rows = table_data[1:]
            df = pd.DataFrame(rows, columns=headers)
        except Exception as e:
            print(f"âš ï¸ è¡¨æ ¼ç»“æ„å¼‚å¸¸ï¼Œé™çº§å¤„ç†: {e}")
            # å¦‚æœåˆ—æ•°å¯¹ä¸ä¸Šï¼ŒPandas ä¼šæŠ¥é”™ï¼Œè¿™é‡Œåšä¸€ä¸ªå…œåº•
            return ""

        # 2. æ ¸å¿ƒæ¸…æ´—ï¼šå¤„ç†ç©ºå€¼ + æš´åŠ›æ¸…é™¤å•å…ƒæ ¼å†…çš„æ¢è¡Œç¬¦
        # è¿™ä¸€æ­¥æ˜¯æ‰‹å†™ä»£ç æœ€éš¾åšåˆ°çš„
        df = df.fillna("")

        # å®šä¹‰æ¸…æ´—å‡½æ•°ï¼šå»æ‰æ¢è¡Œï¼Œå»æ‰å¤šä½™ç©ºæ ¼ï¼Œè½¬ä¹‰ç®¡é“ç¬¦
        def clean_cell(text):
            if not isinstance(text, str):
                return str(text)
            # å°†æ¢è¡Œç¬¦æ›¿æ¢æ‰ï¼Œä¿æŒ Markdown è¡¨æ ¼çš„å•è¡Œç»“æ„
            text = text.replace("\n", " ").replace("\r", "")
            # è½¬ä¹‰ç®¡é“ç¬¦ï¼Œé˜²æ­¢ç ´åè¡¨æ ¼ç»“æ„
            text = text.replace("|", "\|")
            # å»æ‰å¤šä½™çš„è¿ç»­ç©ºæ ¼
            return re.sub(r'\s+', ' ', text).strip()

        # åº”ç”¨æ¸…æ´—åˆ°æ‰€æœ‰å…ƒç´ 
        df = df.map(clean_cell)

        # 3. è¾“å‡º Markdown
        # index=False: ä¸æ˜¾ç¤ºè¡Œå·
        return df.to_markdown(index=False, tablefmt="pipe")

    @staticmethod
    def normalize_table_header(table_data: List[List[Any]]) -> List[List[str]]:
        """
        æ™ºèƒ½æ£€æµ‹å¹¶ä¿®å¤è¡¨æ ¼è¡¨å¤´
        å¦‚æœåˆ¤å®šç¬¬ä¸€è¡Œæ˜¯æ•°æ®ï¼Œåˆ™è‡ªåŠ¨æ³¨å…¥é€šç”¨è¡¨å¤´
        """
        if not table_data:
            return []

        first_row = table_data[0]
        # None å€¼å¤„ç†ä¸ºå­—ç¬¦ä¸²
        safe_first_row = [str(cell) if cell is not None else "" for cell in first_row]

        is_header = True

        # --- è§„åˆ™ 1: é•¿åº¦æ£€æµ‹ ---
        # å¦‚æœç¬¬ä¸€è¡Œé‡Œï¼Œæœ‰ä»»ä½•ä¸€åˆ—çš„å†…å®¹é•¿åº¦è¶…è¿‡ 20 ä¸ªå­—ï¼Œç»å¯¹ä¸å¯èƒ½æ˜¯è¡¨å¤´
        # è¡¨å¤´é€šå¸¸æ˜¯ "å§“å" (2å­—) æˆ– "å½’å±äºæ¯å…¬å¸æ‰€æœ‰è€…çš„å‡€åˆ©æ¶¦" (13å­—)
        for cell_str in safe_first_row:
            if len(cell_str) > 20:
                is_header = False
                break

        # --- è§„åˆ™ 2: æ ‡ç‚¹æ£€æµ‹ ---
        # è¡¨å¤´é‡Œå‡ ä¹ä¸å¯èƒ½å‡ºç°å¥å·(ã€‚)
        if is_header:
            for cell_str in safe_first_row:
                if "ã€‚" in cell_str or "ï¼›" in cell_str:
                    is_header = False
                    break

        # --- æ³¨å…¥é€»è¾‘ ---
        if not is_header:
            print(f"    ğŸ¤– æ£€æµ‹åˆ°æ— å¤´è¡¨æ ¼ï¼Œæ­£åœ¨æ³¨å…¥é»˜è®¤è¡¨å¤´...")
            # é’ˆå¯¹ä½ çš„è¿™ç§ä¸¤åˆ—å¼è§„ç« è¡¨æ ¼ï¼Œ["é¡¹ç›®", "å†…å®¹"] æ˜¯æœ€é€šç”¨çš„
            new_header = [f"åˆ—{i + 1}" for i in range(len(first_row))]

            table_data.insert(0, new_header)

        return table_data

    async def process_images(self, images: List[Dict]) -> List[Document]:
        """
        å¯¹æå–çš„å›¾ç‰‡è¿›è¡Œè¯­ä¹‰åˆ†æï¼ˆå¼‚æ­¥å¹¶å‘ï¼‰
        ä½¿ç”¨ VLM æ¨¡å‹ç”Ÿæˆå›¾ç‰‡æè¿°
        """
        if not images:
            return []

        image_docs = []
        print(f"ğŸ–¼ï¸ å¼€å§‹è¯†åˆ« {len(images)} å¼ å›¾ç‰‡å†…å®¹ (ä½¿ç”¨ VLM)...")

        async def describe_single_image(img_dict):
            """å†…éƒ¨å‡½æ•°ï¼šå¤„ç†å•å¼ å›¾ç‰‡"""
            image_path = img_dict.get("path")
            if not image_path or not os.path.exists(image_path):
                return None

            # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
            async with self.semaphore:
                # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º Base64 ç¼–ç 
                with open(image_path, "rb") as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode("utf-8")

                # æ„é€  Promptï¼ŒæŒ‡å¯¼æ¨¡å‹å¦‚ä½•æè¿°å›¾ç‰‡
                message = HumanMessage(
                    content=[
                        {"type": "text",
                         "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚å¦‚æœæ˜¯æ¶æ„å›¾ï¼Œè¯·è¯´æ˜ç»„ä»¶å…³ç³»ï¼›å¦‚æœæ˜¯æµç¨‹å›¾ï¼Œè¯·è¯´æ˜æ­¥éª¤ï¼›å¦‚æœæ˜¯ç…§ç‰‡ï¼Œè¯·æç‚¼æ ¸å¿ƒä¿¡æ¯ã€‚"},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{encoded_string}"},
                        },
                    ]
                )

                try:
                    # è°ƒç”¨ VLM æ¨¡å‹
                    response = await self.vlm_client.ainvoke([message])
                    description = response.content
                    ocr_result = img_dict.get("description")
                    if ocr_result and ocr_result.strip():
                        # è¿”å› Document å¯¹è±¡ï¼ŒåŒ…å«æè¿°å’Œå…ƒæ•°æ®
                        return Document(
                            page_content=f"[OCRè§£æç»“æœ]:{ocr_result}\n\n[å›¾ç‰‡è¯­ä¹‰åˆ†æ]: {description}",
                            metadata={
                                "type": "image",
                                "image_path": image_path,
                                "page": img_dict.get("page", 0),
                                "source": "pdf_extraction"
                            }
                        )
                    else:
                        return Document(
                            page_content=f"[å›¾ç‰‡è¯­ä¹‰åˆ†æ]: {description}",
                            metadata={
                                "type": "image",
                                "image_path": image_path,
                                "page": img_dict.get("page", 0),
                                "source": "pdf_extraction"
                            }
                        )
                except Exception as e:
                    print(f"âŒ å›¾ç‰‡è§£æå¤±è´¥ ({image_path}): {e}")
                    return None

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [describe_single_image(img) for img in images]
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        results = await asyncio.gather(*tasks)

        # è¿‡æ»¤æ‰å¤±è´¥çš„ç»“æœï¼ˆNoneï¼‰
        image_docs = [doc for doc in results if doc is not None]
        print(f"âœ… å›¾ç‰‡è¯­ä¹‰åŒ–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(image_docs)} æ¡æè¿°ã€‚")
        return image_docs

    # async def process_tables(self, tables: List[Dict], llm_model=None) -> List[Document]:
    #     """
    #     å¯¹æå–çš„è¡¨æ ¼è½¬åŒ–ä¸ºdocumentå¯¹è±¡
    #     """
    #     if not tables:
    #         return []
    #
    #     if llm_model is None:
    #         llm_model = self.vlm_client
    #
    #     table_docs = []
    #     print(f"ğŸ“Š å¼€å§‹åˆ†æ {len(tables)} ä¸ªè¡¨æ ¼...")
    #
    #     async def summarize_single_table(table_dict):
    #         """å†…éƒ¨å‡½æ•°ï¼šå¤„ç†å•ä¸ªè¡¨æ ¼"""
    #         content = table_dict.get("content", "")
    #
    #         # æ„é€  Promptï¼Œè¦æ±‚æ¨¡å‹æ€»ç»“è¡¨æ ¼
    #         prompt = (
    #             "è¯·æ ¹æ®ä»¥ä¸‹è¡¨æ ¼çš„Markdownå†…å®¹ï¼Œç”Ÿæˆä¸€æ®µç®€æ´çš„æ–‡æœ¬æ‘˜è¦ã€‚"
    #             "æ‘˜è¦åº”åŒ…å«è¡¨æ ¼çš„ä¸»è¦ä¸»é¢˜ã€åˆ—åå«ä¹‰ä»¥åŠå…³é”®æ•°æ®ç‚¹ã€‚"
    #             "ä¸è¦è¾“å‡ºæ ‡ç­¾ï¼Œåªè¾“å‡ºçº¯æ–‡æœ¬æè¿°ã€‚"
    #             f"\n\nè¡¨æ ¼å†…å®¹:\n{content}"
    #         )
    #
    #         try:
    #             # ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘
    #             async with self.semaphore:
    #                 # è°ƒç”¨ LLM
    #                 response = await llm_model.ainvoke([HumanMessage(content=prompt)])
    #                 summary = response.content
    #
    #             # è¿”å›åŒ…å«æ‘˜è¦çš„ Document
    #             return Document(
    #                 page_content=summary,
    #                 metadata={
    #                     "type": "table",
    #                     "page": table_dict.get("page", 0),
    #                     "markdown_content": content,
    #                     "source": "pdf_extraction"
    #                 }
    #             )
    #         except Exception as e:
    #             print(f"âŒ è¡¨æ ¼æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
    #             # å³ä½¿æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä¹Ÿè¿”å›åŸå§‹è¡¨æ ¼å†…å®¹
    #             return Document(
    #                 page_content=content,
    #                 metadata={
    #                     "type": "table",
    #                     "page": table_dict.get("page", 0),
    #                     "markdown_content": content,
    #                     "source": "pdf_extraction",
    #                     "error": str(e)
    #                 }
    #             )
    #
    #     tasks = [summarize_single_table(table) for table in tables]
    #     results = await asyncio.gather(*tasks)
    #
    #     table_docs = [doc for doc in results if doc is not None]
    #     print(f"âœ… è¡¨æ ¼åˆ†æå®Œæˆï¼Œå…±ç”Ÿæˆ {len(table_docs)} æ¡æ‘˜è¦ã€‚")
    #     return table_docs

    @staticmethod
    def to_documents(parse_result: Dict) -> List[Document]:
        """
        å°†è§£æç»“æœè½¬æ¢ä¸ºæ ‡å‡†çš„ LangChain Document å¯¹è±¡åˆ—è¡¨
        æ–¹ä¾¿åç»­å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“æˆ–ç”¨äº RAG æ£€ç´¢
        """
        documents = []
        file_name = parse_result["metadata"].get("file_name", "Unknown")
        file_hash = parse_result["metadata"].get("file_hash", "")
        # å°†æ¯ä¸ªæ–‡æœ¬å—è½¬æ¢ä¸ºä¸€ä¸ª Document
        for text_block in parse_result.get("texts", []):
            documents.append(Document(
                page_content=text_block["text"],
                metadata={
                    "file_name": file_name,
                    "file_hash": file_hash,
                    "type": text_block.get("type", "Text"),
                    "page": text_block.get("page", 0),
                    "source": "pdf_extraction"
                }
            ))

        # å°†æ¯ä¸ªè¡¨æ ¼è½¬æ¢ä¸ºä¸€ä¸ª Document
        for table in parse_result.get("tables", []):
            documents.append(Document(
                page_content=table["content"],
                metadata={
                    "file_hash": file_hash,
                    "type": "table",
                    "page": table.get("page", 0),
                    "format": "markdown",
                    "source": "pdf_extraction"
                }
            ))

        # for image in parse_result.get("saved_images", []):
        #     documents.append(Document(
        #         page_content=image["description"],
        #         metadata={
        #             "img_name": image.get("name", ""),
        #             "img_path": image.get("path", ""),
        #             "description": image.get("description", ""),
        #             "page": image.get("page", 0),
        #             "source": "pdf_extraction",
        #             "type": "image",
        #         }
        #     ))

        return documents


# ==================== å·¥å‚å‡½æ•° ====================
def get_pdf_parser(prefer_gpu: bool = True):
    """
    æ™ºèƒ½å·¥å‚å‡½æ•°ï¼šè·å–æœ€åˆé€‚çš„ PDF è§£æå™¨

    Args:
        prefer_gpu: æ˜¯å¦ä¼˜å…ˆå°è¯•ä½¿ç”¨ GPU ç‰ˆæœ¬çš„è§£æå™¨ (Marker)

    Returns:
        è§£æå™¨å®ä¾‹ (AdvancedPDFParser æˆ– RapidPDFParser)
    """
    if prefer_gpu:
        try:
            # å°è¯•å¯¼å…¥ PyTorch å¹¶æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨
            import torch
            if torch.cuda.is_available():
                # å¦‚æœæœ‰ GPUï¼Œå¯¼å…¥å¹¶è¿”å›é«˜çº§è§£æå™¨(åŸºäº Marker)
                from parser import AdvancedPDFParser
                return AdvancedPDFParser()
        except Exception as e:
            # å¦‚æœå‡ºé”™ï¼ˆå¦‚æœªå®‰è£… torch æˆ–æ˜¾å­˜ä¸è¶³ï¼‰ï¼Œæ‰“å°è­¦å‘Šå¹¶å›é€€
            print(f"âš ï¸ GPU è§£æå™¨ä¸å¯ç”¨: {e}")

    # å¦‚æœæ²¡æœ‰ GPU æˆ–åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°è½»é‡çº§è§£æå™¨ (RapidOCR)
    return RapidPDFParser()
