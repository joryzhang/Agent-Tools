"""
é«˜çº§ PDF è§£æå™¨ - ä½¿ç”¨ Marker + Surya
æ³¨æ„ï¼šæ­¤è§£æå™¨éœ€è¦é«˜æ€§èƒ½ GPU æ”¯æŒ
"""
# ==================== å¯¼å…¥å¿…è¦çš„æ ‡å‡†åº“ ====================
import asyncio  # å¼‚æ­¥å¹¶å‘åº“
import base64  # å›¾ç‰‡ Base64 ç¼–ç 
import hashlib  # è®¡ç®— MD5 å“ˆå¸Œ
import re  # æ­£åˆ™è¡¨è¾¾å¼
import os  # æ“ä½œç³»ç»Ÿæ¥å£
import sys  # ç³»ç»Ÿå‚æ•°
import json  # JSON å¤„ç†
from typing import List, Any, Dict  # ç±»å‹æç¤º
from pathlib import Path  # é¢å‘å¯¹è±¡çš„æ–‡ä»¶è·¯å¾„

# ==================== ä¿®å¤ Windows ç»ˆç«¯ç¼–ç é—®é¢˜ ====================
# Windows ç»ˆç«¯é»˜è®¤å¯èƒ½ä¸æ”¯æŒ emoji æ˜¾ç¤ºï¼Œå¼ºåˆ¶è®¾ç½®ä¸º utf-8
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')

# ==================== å¯¼å…¥ç›¸å…³ä¾èµ–åº“ ====================
from langchain_core.documents import Document  # LangChain æ–‡æ¡£å¯¹è±¡
from langchain_core.messages import HumanMessage  # LangChain æ¶ˆæ¯å¯¹è±¡
# Marker åº“ï¼šç”¨äºå°† PDF è½¬æ¢ä¸ºé«˜è´¨é‡ Markdown
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict

# ==================== å¯¼å…¥é¡¹ç›®è‡ªå®šä¹‰æ¨¡å— ====================
from models import get_openai_virtual_model_client  # AI æ¨¡å‹å®¢æˆ·ç«¯
from app.config import DATA_DIR, IMG_OUTPUT_DIR  # ç›®å½•é…ç½®
from dotenv import load_dotenv  # ç¯å¢ƒå˜é‡åŠ è½½

# åŠ è½½ .env ç¯å¢ƒå˜é‡
load_dotenv()


class AdvancedPDFParser:
    """
    é«˜çº§ PDF è§£æå™¨ç±»
    ä¸»è¦åŠŸèƒ½ï¼š
    1. ä½¿ç”¨ Marker æ¨¡å‹æ·±åº¦è§£æ PDF å¸ƒå±€
    2. ä½¿ç”¨ Surya æ¨¡å‹è¿›è¡Œ OCR å’Œæ–‡æœ¬è¡Œæ£€æµ‹
    3. ç”Ÿæˆé«˜è´¨é‡çš„ Markdown è¾“å‡º
    4. æå–å›¾ç‰‡å’Œè¡¨æ ¼
    """

    def __init__(self):
        """åˆå§‹åŒ–è§£æå™¨å¹¶åŠ è½½ AI æ¨¡å‹"""
        # é¢„åŠ è½½ Marker éœ€è¦çš„æ‰€æœ‰æ¨¡å‹ (åŒ…æ‹¬ Surya)
        # æ³¨æ„ï¼šè¿™ä¼šå ç”¨å‡  GB çš„æ˜¾å­˜/å†…å­˜ï¼Œå»ºè®®åœ¨åº”ç”¨å¯åŠ¨æ—¶åªåˆå§‹åŒ–ä¸€æ¬¡
        print("ğŸ”„ æ­£åœ¨åŠ è½½ Marker+Surya æ¨¡å‹...")

        # create_model_dict() ä¼šä¸‹è½½å¹¶åŠ è½½æ‰€éœ€çš„ PyTorch æ¨¡å‹
        # åŒ…æ‹¬å¸ƒå±€åˆ†æã€OCRã€å…¬å¼è¯†åˆ«ç­‰æ¨¡å‹
        self.artifact_dict = create_model_dict()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

        # é…ç½®ç›®å½•
        self.img_dir = str(IMG_OUTPUT_DIR)
        self.cache_dir = DATA_DIR / "cache"
        os.makedirs(self.cache_dir, exist_ok=True)
        os.makedirs(self.img_dir, exist_ok=True)

        # åˆå§‹åŒ– VLM å®¢æˆ·ç«¯ (ç”¨äºå›¾ç‰‡ç†è§£)
        self.vlm_client = get_openai_virtual_model_client()
        # é™åˆ¶å¹¶å‘æ•°ä¸º 5
        self.semaphore = asyncio.Semaphore(5)

    def _get_file_hash(self, file_path: str) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„ MD5 å€¼ä½œä¸ºå”¯ä¸€æ ‡è¯†
        ç”¨äºç¼“å­˜æ–‡ä»¶åï¼Œé¿å…é‡å¤è§£æåŒä¸€æ–‡ä»¶
        """
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def parse_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """
        ä½¿ç”¨ Marker + Surya è§£æ PDF
        è¿™æ˜¯è§£æçš„ä¸»è¦å…¥å£å‡½æ•°
        
        Returns:
            å­—å…¸åŒ…å«:
            - markdown: å®Œæ•´çš„ Markdown æ–‡æœ¬
            - images: å›¾ç‰‡å¯¹è±¡å­—å…¸
            - metadata: PDF å…ƒæ•°æ®
            - tables: æå–çš„è¡¨æ ¼åˆ—è¡¨
            - texts: æå–çš„æ–‡æœ¬å—åˆ—è¡¨
        """
        file_hash = self._get_file_hash(pdf_path)
        cache_path = self.cache_dir / f"{file_hash}_marker.json"

        # -------------------- æ£€æŸ¥ç¼“å­˜ --------------------
        if cache_path.exists():
            print(f"â™»ï¸ å‘ç°ç¼“å­˜ï¼Œç›´æ¥è¯»å–: {file_hash}")
            with open(cache_path, "r", encoding="utf-8") as f:
                cached = json.load(f)
                # images æ— æ³•åºåˆ—åŒ–åˆ° JSONï¼Œä»ç¼“å­˜è¯»å–æ—¶ä¸ºç©º
                cached["images"] = {}
                return cached

        print(f"ğŸš€ ä½¿ç”¨ Marker+Surya å¼•æ“è§£æ: {pdf_path}")

        # -------------------- åˆ›å»º PDF è½¬æ¢å™¨ --------------------
        # ä½¿ç”¨é¢„åŠ è½½çš„æ¨¡å‹å­—å…¸åˆå§‹åŒ–è½¬æ¢å™¨
        converter = PdfConverter(
            artifact_dict=self.artifact_dict,
            config={
                # è®¾ç½® OCR è¯­è¨€ï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡
                "languages": ["Chinese", "English"],
            }
        )

        # -------------------- æ‰§è¡Œè½¬æ¢ --------------------
        # è¿™ä¸€æ­¥ä¼šè°ƒç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹è¿›è¡Œå¤æ‚çš„é¡µé¢åˆ†æ
        # ä¹Ÿæ˜¯æœ€è€—æ—¶çš„ä¸€æ­¥
        rendered = converter(pdf_path)
        # rendered æ˜¯ä¸€ä¸ª MarkdownOutput å¯¹è±¡ï¼ŒåŒ…å« markdown, images, metadata
        full_text = rendered.markdown
        images = rendered.images if hasattr(rendered, 'images') else {}
        out_metadata = rendered.metadata if hasattr(rendered, 'metadata') else {}

        # -------------------- ä¿å­˜å›¾ç‰‡åˆ°ç£ç›˜ --------------------
        saved_images = []
        if images:
            for img_name, img_data in images.items():
                img_path = os.path.join(self.img_dir, f"{file_hash}_{img_name}")
                # æ ¹æ®å›¾ç‰‡å¯¹è±¡ç±»å‹ä¿å­˜
                if hasattr(img_data, 'save'):
                    # å¦‚æœæ˜¯ PIL Image å¯¹è±¡
                    img_data.save(img_path)
                elif isinstance(img_data, bytes):
                    # å¦‚æœæ˜¯äºŒè¿›åˆ¶æ•°æ®
                    with open(img_path, 'wb') as f:
                        f.write(img_data)

                saved_images.append({
                    "name": img_name,
                    "path": img_path
                })
                print(f"  ğŸ“· ä¿å­˜å›¾ç‰‡: {img_path}")

        # -------------------- åå¤„ç†ï¼šæå–è¡¨æ ¼å’Œæ–‡æœ¬ --------------------
        # Marker è¾“å‡ºçš„æ˜¯çº¯ Markdownï¼Œæˆ‘ä»¬éœ€è¦ä»ä¸­è§£æå‡ºç»“æ„åŒ–æ•°æ®

        # ä» Markdown ä¸­æå–è¡¨æ ¼
        tables = self._extract_tables_from_markdown(full_text)

        # ä» Markdown ä¸­æå–æ–‡æœ¬å— (åˆ†æ®µ)
        texts = self._extract_text_blocks(full_text)

        result = {
            "markdown": full_text,
            "images": images,
            "saved_images": saved_images,
            "metadata": out_metadata if isinstance(out_metadata, dict) else {},
            "tables": tables,
            "texts": texts
        }

        # -------------------- å†™å…¥ç¼“å­˜ --------------------
        # å‡†å¤‡å¯åºåˆ—åŒ–çš„ç¼“å­˜æ•°æ® (å»é™¤ images å¯¹è±¡)
        cache_data = {
            "markdown": full_text,
            "saved_images": saved_images,
            "metadata": out_metadata if isinstance(out_metadata, dict) else {},
            "tables": tables,
            "texts": texts
        }
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

        print(f"âœ… è§£æå®Œæˆ: æ–‡æœ¬å—[{len(texts)}], è¡¨æ ¼[{len(tables)}], å›¾ç‰‡[{len(saved_images)}]")
        return result

    def _extract_tables_from_markdown(self, markdown_text: str) -> List[Dict]:
        """
        è¾…åŠ©å‡½æ•°ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä» Markdown æ–‡æœ¬ä¸­æå–è¡¨æ ¼
        è¿”å›è¡¨æ ¼åˆ—è¡¨ï¼Œæ¯ä¸ªè¡¨æ ¼åŒ…å«å†…å®¹å’Œç´¢å¼•
        """
        tables = []
        # åŒ¹é… Markdown è¡¨æ ¼çš„æ­£åˆ™: 
        # ä»¥ | å¼€å¤´ï¼Œç¬¬äºŒè¡ŒåŒ…å« |---| æˆ– |:---| ç­‰å¯¹é½æ ‡è¯†
        table_pattern = r'(\|[^\n]+\|\n\|[-:\s|]+\|\n(?:\|[^\n]+\|\n?)+)'

        matches = re.findall(table_pattern, markdown_text)
        for i, match in enumerate(matches):
            tables.append({
                "index": i,
                "content": match.strip(),
                "format": "markdown"
            })

        return tables

    def _extract_text_blocks(self, markdown_text: str) -> List[Dict]:
        """
        è¾…åŠ©å‡½æ•°ï¼šå°† Markdown æ–‡æœ¬æŒ‰æ®µè½åˆ†å‰²ä¸ºæ–‡æœ¬å—
        å¹¶ç®€å•çš„è¯†åˆ«æ®µè½ç±»å‹ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€æ™®é€šæ–‡æœ¬ï¼‰
        """
        texts = []

        # æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½
        paragraphs = markdown_text.split('\n\n')

        for i, para in enumerate(paragraphs):
            para = para.strip()
            if not para:
                continue

            # åˆ¤æ–­ç±»å‹
            if para.startswith('#'):
                text_type = "Title"  # æ ‡é¢˜
            elif para.startswith('- ') or para.startswith('* ') or re.match(r'^\d+\.', para):
                text_type = "ListItem"  # åˆ—è¡¨é¡¹
            elif para.startswith('|'):
                # è·³è¿‡è¡¨æ ¼ï¼Œå› ä¸ºå·²ç»åœ¨ _extract_tables_from_markdown ä¸­å¤„ç†
                continue
            elif para.startswith('!['):
                # è·³è¿‡å›¾ç‰‡å¼•ç”¨
                continue
            else:
                text_type = "NarrativeText"  # æ™®é€šå™è¿°æ–‡æœ¬

            texts.append({
                "index": i,
                "type": text_type,
                "text": para
            })

        return texts

    async def process_images(self, images: List[Dict]) -> List[Document]:
        """
        å›¾ç‰‡å¤„ç†æµæ°´çº¿ï¼šå›¾ç‰‡è·¯å¾„ -> Base64ç¼–ç  -> VLM -> æ–‡æœ¬æè¿°
        æ­¤æ–¹æ³•ç”¨äºå¯¹æå–å‡ºçš„å›¾ç‰‡è¿›è¡Œæ·±å…¥çš„è¯­ä¹‰åˆ†æ
        """
        if not images:
            return []

        image_docs = []
        print(f"ğŸ–¼ï¸ å¼€å§‹è¯†åˆ« {len(images)} å¼ å›¾ç‰‡å†…å®¹ (ä½¿ç”¨ VLM)...")

        async def describe_single_image(img_dict):
            """å†…éƒ¨å‡½æ•°å¤„ç†å•å¼ å›¾ç‰‡"""
            image_path = img_dict.get("path")
            if not image_path or not os.path.exists(image_path):
                return None

            async with self.semaphore:
                # å°†å›¾ç‰‡è½¬ä¸º Base64
                with open(image_path, "rb") as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode("utf-8")

                # æ„é€ å¤šæ¨¡æ€æ¶ˆæ¯
                message = HumanMessage(
                    content=[
                        {"type": "text",
                         "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ã€‚å¦‚æœæ˜¯æ¶æ„å›¾ï¼Œè¯·è¯´æ˜ç»„ä»¶å…³ç³»ï¼›å¦‚æœæ˜¯æµç¨‹å›¾ï¼Œè¯·è¯´æ˜æ­¥éª¤ï¼›å¦‚æœæ˜¯ç…§ç‰‡ï¼Œè¯·æç‚¼æ ¸å¿ƒä¿¡æ¯ã€‚"},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{encoded_string}"},
                        },
                    ]
                )

                try:
                    # è°ƒç”¨ VLM æ¨¡å‹
                    response = await self.vlm_client.ainvoke([message])
                    description = response.content

                    return Document(
                        page_content=f"[å›¾ç‰‡è¯­ä¹‰åˆ†æ]: {description}",
                        metadata={
                            "type": "image",
                            "image_path": image_path,
                            "source": "pdf_extraction"
                        }
                    )
                except Exception as e:
                    print(f"âŒ å›¾ç‰‡è§£æå¤±è´¥ ({image_path}): {e}")
                    return None

        # å¹¶å‘æ‰§è¡Œå›¾ç‰‡åˆ†æä»»åŠ¡
        tasks = [describe_single_image(img) for img in images]
        results = await asyncio.gather(*tasks)

        image_docs = [doc for doc in results if doc is not None]
        print(f"âœ… å›¾ç‰‡è¯­ä¹‰åŒ–å®Œæˆï¼Œå…±ç”Ÿæˆ {len(image_docs)} æ¡æè¿°ã€‚")
        return image_docs

    async def process_tables(self, tables: List[Dict], llm_model=None) -> List[Document]:
        """
        è¡¨æ ¼å¤„ç†æµæ°´çº¿ï¼šMarkdownè¡¨æ ¼ -> LLM -> åˆ†ææ‘˜è¦
        æ­¤æ–¹æ³•ç”¨äºç†è§£è¡¨æ ¼æ•°æ®ï¼Œç”Ÿæˆè‡ªç„¶è¯­è¨€æ‘˜è¦
        """
        if not tables:
            return []

        if llm_model is None:
            llm_model = self.vlm_client

        table_docs = []
        print(f"ğŸ“Š å¼€å§‹åˆ†æ {len(tables)} ä¸ªè¡¨æ ¼...")

        async def summarize_single_table(table_dict):
            """å†…éƒ¨å‡½æ•°å¤„ç†å•ä¸ªè¡¨æ ¼"""
            content = table_dict.get("content", "")

            # æ„é€  Prompt æŒ‡å¯¼æ¨¡å‹æ€»ç»“è¡¨æ ¼
            prompt = (
                "è¯·æ ¹æ®ä»¥ä¸‹è¡¨æ ¼çš„Markdownå†…å®¹ï¼Œç”Ÿæˆä¸€æ®µç®€æ´çš„æ–‡æœ¬æ‘˜è¦ã€‚"
                "æ‘˜è¦åº”åŒ…å«è¡¨æ ¼çš„ä¸»è¦ä¸»é¢˜ã€åˆ—åå«ä¹‰ä»¥åŠå…³é”®æ•°æ®ç‚¹ã€‚"
                "ä¸è¦è¾“å‡ºæ ‡ç­¾ï¼Œåªè¾“å‡ºçº¯æ–‡æœ¬æè¿°ã€‚"
                f"\n\nè¡¨æ ¼å†…å®¹:\n{content}"
            )

            try:
                # è°ƒç”¨ LLM
                async with self.semaphore:
                    response = await llm_model.ainvoke([HumanMessage(content=prompt)])
                    summary = response.content

                return Document(
                    page_content=summary,
                    metadata={
                        "type": "table",
                        "markdown_content": content,
                        "source": "pdf_extraction"
                    }
                )
            except Exception as e:
                print(f"âŒ è¡¨æ ¼æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
                # å¤±è´¥æ—¶è¿”å›åŸå§‹å†…å®¹
                return Document(
                    page_content=content,
                    metadata={
                        "type": "table",
                        "markdown_content": content,
                        "source": "pdf_extraction",
                        "error": str(e)
                    }
                )

        # å¹¶å‘æ‰§è¡Œè¡¨æ ¼æ‘˜è¦ä»»åŠ¡
        tasks = [summarize_single_table(table) for table in tables]
        results = await asyncio.gather(*tasks)

        table_docs = [doc for doc in results if doc is not None]
        print(f"âœ… è¡¨æ ¼åˆ†æå®Œæˆï¼Œå…±ç”Ÿæˆ {len(table_docs)} æ¡æ‘˜è¦ã€‚")
        return table_docs

    def to_documents(self, parse_result: Dict) -> List[Document]:
        """
        å°†è§£æç»“æœå­—å…¸è½¬æ¢ä¸º LangChain Document åˆ—è¡¨
        ä»¥ä¾¿äºåç»­å­˜å…¥å‘é‡æ•°æ®åº“
        """
        documents = []

        # æ·»åŠ æ–‡æœ¬å—
        for text_block in parse_result.get("texts", []):
            documents.append(Document(
                page_content=text_block["text"],
                metadata={
                    "type": text_block["type"],
                    "source": "pdf_extraction"
                }
            ))

        # æ·»åŠ è¡¨æ ¼ (åŸå§‹ Markdown å†…å®¹ä½œä¸ºæ–‡æœ¬)
        for table in parse_result.get("tables", []):
            documents.append(Document(
                page_content=table["content"],
                metadata={
                    "type": "table",
                    "format": "markdown",
                    "source": "pdf_extraction"
                }
            ))

        return documents
